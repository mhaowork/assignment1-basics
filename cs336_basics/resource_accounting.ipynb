{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92dd0bf",
   "metadata": {},
   "source": [
    "## (a) Suppose we constructed our model using this configuration. How many trainable parameters\n",
    "would our model have? Assuming each parameter is represented using single-precision floating\n",
    "point, how much memory is required to just load this model?\n",
    "\n",
    "Deliverable: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23dac4e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Counting parameters\n",
    "Input Embedding: vocab_size x d_model\n",
    "\n",
    "\n",
    "Transformer block:\n",
    "- rms norm -> d_model\n",
    "- q k v o proj weights-> d_model * d_model * 4\n",
    "- rms norm -> d_model\n",
    "- ffn w1 w2 w3 -> d_ff * d_model * 3\n",
    "\n",
    "Transformer block total: (d_model + d_model * d_model * 4 + d_model + d_ff * d_model * 3)\n",
    "\n",
    "rms norm final -> d_model\n",
    "\n",
    "output linear -> d_model * vocab_size\n",
    "\n",
    "softmax -> 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbad6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Consider GPT-2 XL, which has the following configuration:\n",
    "vocab_size = 50257\n",
    "context_length = 1024\n",
    "num_layers = 48\n",
    "d_model = 1600\n",
    "\n",
    "num_heads = 25\n",
    "d_ff = 6400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41b4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1635537600\n"
     ]
    }
   ],
   "source": [
    "total_params = (\n",
    "    vocab_size * d_model\n",
    "    + num_layers * (d_model + d_model * d_model * 4 + d_model + d_ff * d_model * 3)\n",
    "    + d_model\n",
    "    + d_model * vocab_size\n",
    ")\n",
    "\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a7bbee",
   "metadata": {},
   "source": [
    "\n",
    "## (b) Identify the matrix multiplies required to complete a forward pass of our GPT-2 XL-shaped\n",
    "model. How many FLOPs do these matrix multiplies require in total? Assume that our input\n",
    "sequence has context_length tokens.\n",
    "\n",
    "**Deliverable**: A list of matrix multiplies (with descriptions), and the total number of FLOPs\n",
    "required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f4123",
   "metadata": {},
   "source": [
    "### Input embedding\n",
    "\n",
    "None \n",
    "\n",
    "\n",
    "### Transformer Block \n",
    "\n",
    "1. RMS Norm:\n",
    "\n",
    "None\n",
    "\n",
    "2. MHSA w/ RoPE\n",
    "\n",
    "Projection: q k v o so 4 times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "233c16f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006632960000\n"
     ]
    }
   ],
   "source": [
    "proj = context_length * d_model * d_model * 4 * 2 # 2 flops per multily-add op\n",
    "\n",
    "print(proj * num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa583047",
   "metadata": {},
   "source": [
    "\n",
    "Attention: Q @ K & softmax @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac2704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322122547200\n",
      "1328755507200\n"
     ]
    }
   ],
   "source": [
    "attn_qk = context_length * context_length * d_model * 2\n",
    "\n",
    "# (... context_lenght context_length) @ (context_length d_model)\n",
    "attn_v = context_length * d_model * context_length * 2\n",
    "\n",
    "attn = attn_qk + attn_v\n",
    "\n",
    "print((attn + proj) * num_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d9b77b",
   "metadata": {},
   "source": [
    "\n",
    "RoPE: None\n",
    "\n",
    "\n",
    "3. FFN (SwiGLU): w1x, w2x, w3x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bfcdab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3019898880000\n"
     ]
    }
   ],
   "source": [
    "ffn = context_length * d_ff * d_model * 3 * 2\n",
    "\n",
    "print(ffn * num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98074a05",
   "metadata": {},
   "source": [
    "\n",
    "### Output embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71877ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_linear = context_length * d_model * vocab_size * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11943db3",
   "metadata": {},
   "source": [
    "### Total Flops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d74f9fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4430995456000\n"
     ]
    }
   ],
   "source": [
    "mhsa_flops = proj + attn\n",
    "transformer_block_flops = mhsa_flops + ffn\n",
    "total_flops = num_layers * transformer_block_flops + output_linear\n",
    "\n",
    "print(total_flops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b8fa39",
   "metadata": {},
   "source": [
    "## (c) Based on your analysis above, which parts of the model require the most FLOPs?\n",
    "\n",
    "\n",
    "**FFN** See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb23f61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62,914,560,000\n",
      "6,710,886,400\n",
      "20,971,520,000\n"
     ]
    }
   ],
   "source": [
    "print(f\"{ffn:,}\")\n",
    "print(f\"{attn:,}\")\n",
    "print(f\"{proj:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e57de0",
   "metadata": {},
   "source": [
    "## (d) Repeat your analysis with GPT-2 small (12 layers, 768 d_model, 12 heads), GPT-2 medium (24 layers, 1024 d_model, 16 heads), and GPT-2 large (36 layers, 1280 d_model, 20 heads). \n",
    "\n",
    "As the model size increases, which parts of the Transformer LM take up proportionally more or less of\n",
    "the total FLOPs?\n",
    "\n",
    "**Deliverable**: For each model, provide a breakdown of model components and its associated\n",
    "FLOPs (as a proportion of the total FLOPs required for a forward pass). In addition, provide a\n",
    "one-to-two sentence description of how varying the model size changes the proportional FLOPs\n",
    "of each component."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
